---
title: "Mandatory Assignment 1 AEF"
author: 'ncx951 & dsc579'
date: "`r Sys.Date()`"
output: html_document

---



```{r include=FALSE}
# Packages

# Graphics
library(ggplot2)
library(cowplot)
library(lattice)
library(hexbin)
library(gridExtra)
library(reshape2)
library(grid)
library(tidyverse)
library(tidymodels)


# Table formatting
library(xtable)
library(pander)
library(kableExtra)
library(vtable)

# Machine Learning
library(keras)
library(tensorflow)
library(Metrics)

# Other
library(readr)
library(dplyr)
library(lubridate)
library(RSQLite)
library(knitr)



```


# Excercise 1


```{r cache=TRUE}
# Load in the dataset
tidy_finance <- dbConnect(
  SQLite(),
  "tidy_finance_ML.sqlite",
  extended_types = TRUE
)


stock_characteristics_monthly <- tbl(tidy_finance, "stock_characteristics_monthly") |> collect()
```




```{r cache=TRUE}

# Filtering out characteristics and only include data from 2005-01-01
stock_characteristics_monthly <- stock_characteristics_monthly |>
               select(permno, month, ret_excess, mktcap_lag, sic2, 
                      macro_bm, macro_ntis, macro_tbl, macro_dfy, 
                      characteristic_mom1m, characteristic_mom12m, characteristic_chmom, characteristic_maxret, characteristic_mvel1) |>     
               filter(month >= "2005-01-01") |>
               drop_na()

# Renaming column names according to the names from the Gu et al paper

colnames(stock_characteristics_monthly) <- c("permno", "month", "ret_excess", "mktcap_lag", "sic2", 
                                             "m_bm", "m_ntis", "m_tbl", "m_dfy",
                                             "c_mom1m", "c_mom12m", "c_chmom", "c_maxret", "c_mvel1")

# Turn sic2 into a factor instead of numerical value
stock_characteristics_monthly$sic2 <- as.factor(stock_characteristics_monthly$sic2)

# Convert month column to Date format
stock_characteristics_monthly$month <- as.Date(stock_characteristics_monthly$month)

dbDisconnect(tidy_finance)
remove(tidy_finance)

```

The dataset includes observations from January 1st 2005. 

We make summary statistics below

```{r fig.height=5, fig.width=10}


# Counting how many firms (permno) thats in each industry (sic2) for each month
no_industry <- select(stock_characteristics_monthly, permno, month, sic2) |>
                group_by(month, sic2) |>
                summarise(n = n())
 
# Creates a stacked barplot
ggplot(no_industry, aes(fill=sic2, y=n, x=month)) +
  geom_bar(position = "stack", stat = "identity") +
  ggtitle("Number of unique firms (permno) in each industry classification (sic2) for each month") + 
  xlab("Time") + 
  ylab("No. firms") +
  scale_fill_discrete(name = "Industry classifications (sic2)")

```






Summary table

```{r}


stock_characteristics_monthly_summary <- stock_characteristics_monthly |>
  select(-permno, -month, -sic2) |>
  pivot_longer(cols = everything(), names_to = "Variables", values_to = "Value") |>
  group_by(Variables) |>
  summarize_all(list(mean = mean, 
                     sd = sd,
                     min = min,
                     median = median,
                     max = max)) |>
  knitr::kable(booktabs = TRUE, digits = 2, caption = "Summary statistics of the predictors") |> 
  kable_paper("hover", full_width = T) |>  
  group_rows("Stock Characteristics", 1, 5) |> 
  group_rows("Macro Characteristics", 6, 9) |> 
  group_rows("Initial Variables", 10, 11)

stock_characteristics_monthly_summary

```




# Excercise 2

Gu et al. (2020) describe an assetâ€™s excess return as an additive prediction error model the following way:

$$r_{i,t+1} = E_t(r_{i,t+1}) + \varepsilon_{i,t+1} \quad \text{where} \quad E_t(r_{i,t+1}) = g(z_{i,t})$$
Where $g(z_{i,t})$ is a function of the $P$-dimensional vector $z_{i,t}$ of predictor variables. 

As stated in the paper this would be considered a very flexible model, however it imposes some important restrictions. First the $g(\cdot)$ function depends neither on the individual stock or time period. It thus leverages of the information from the entire dataset. The functional form thus doesn't adjust by time period or for specific stocks. For example one variable could have a larger explanatory power in the beginning of the time period but much lesser towards the end. The effect from the predictor would still remain constant, and could also relate to overfitting the model.  The model also assumes that the prediction error $\varepsilon_{i,t+1}$ is additive and independent of the predictor variables, which may not hold in reality.


# Excercise 3

Making the train-validation-test split with 20% of the last observations as test data and make 20% of the train data into validation.


```{r}

# Splitting into a 80/20 test-train split

split_train <- initial_time_split(
  stock_characteristics_monthly |>
    select(-permno, -month),
  prop = 4/5
)

# Further splitting 20% of training data into validation

new_split_train <- initial_time_split(
  training(split_train), 
  prob = 4/5
)


# Making the recipe        
rec <- recipe(ret_excess ~ ., data = new_split_train) |>
  step_interact(terms = ~contains("c_"):contains("m_")) |>
  step_dummy(sic2, one_hot = TRUE) |>
  step_normalize(all_predictors()) |>
  step_center(ret_excess, skip = TRUE)


# Preprocess the training data
prep_train <- prep(rec, training(new_split_train))
data_train <- bake(prep_train, new_data = training(new_split_train))

# Preprocess the validation data

prep_val <- prep(rec, testing(new_split_train))
data_val <- bake(prep_train, new_data = testing(new_split_train))

# Preprocess the testing data
prep_test <- prep(rec, testing(split_train))
data_test <- bake(prep_train, new_data = testing(split_train))


```


# Excercise 4

Neural Network

```{r}



lambda <- 0.0001
dropout_rate <- 0.05

NN_model <- keras_model_sequential() |>
    layer_dense(units = 10, activation = "sigmoid", input_shape = 100, kernel_regularizer = regularizer_l2(lambda)) |>
    layer_dropout(rate=dropout_rate) |>
    layer_dense(units = 10, activation = "sigmoid", kernel_regularizer = regularizer_l2(lambda)) |>
    layer_dropout(rate=dropout_rate) |>
    layer_dense(units = 10, activation = "sigmoid", kernel_regularizer = regularizer_l2(lambda)) |>
    layer_dropout(rate=dropout_rate) |>
    layer_dense(units = 1, activation = "linear") |>

  compile(
    loss = 'mse',
    optimizer = optimizer_rmsprop(learning_rate=0.001, rho=0.9)    
)
 
 
NN_fit <- NN_model |>
  fit(
    x = data_train |>
      select(-ret_excess) |>
      as.matrix(),
    y = data_train |>
      pull(ret_excess),
    epochs = 20, batch_size = 128, verbose = FALSE
  )


```


```{r}

NN_pred <- NN_model |>
  predict(data_val |>
          select(-ret_excess) |>
          as.matrix()
            )

# Compute MSE
mse_NN <- mse(NN_pred, data_val$ret_excess)
print(paste0("MSE: ", mse_NN))
```




